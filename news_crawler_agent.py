"""åŸºäº LangChain çš„æ™ºèƒ½æ–°é—»çˆ¬è™« Agent"""
import os
import json
import re
import logging
from typing import List, Dict, Callable, Any
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import random
from functools import wraps
from sqlalchemy import create_engine, text
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from langchain_core.runnables import RunnablePassthrough
import urllib3
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env.dev')

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®logger
logger = logging.getLogger(__name__)

def retry_decorator(max_retries=3, initial_delay=2, backoff_factor=2, exceptions=(Exception,)):
    """é‡è¯•è£…é¥°å™¨
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹å»¶è¿Ÿæ—¶é—´(ç§’)
        backoff_factor: é€€é¿å› å­ï¼Œç”¨äºè®¡ç®—æŒ‡æ•°é€€é¿
        exceptions: éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            delay = initial_delay
            
            while retries <= max_retries:
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    retries += 1
                    if retries > max_retries:
                        logger.error(f"æœ€å¤§é‡è¯•æ¬¡æ•°å·²ç”¨å°½ï¼Œæœ€ç»ˆå¼‚å¸¸: {str(e)}")
                        raise
                    
                    # æ·»åŠ ä¸€äº›éšæœºæ€§é¿å…åŒæ—¶è¯·æ±‚
                    jitter = random.uniform(0, 0.5) * delay
                    sleep_time = delay + jitter
                    
                    logger.warning(f"è¯·æ±‚å¤±è´¥ï¼Œè¿›è¡Œç¬¬ {retries} æ¬¡é‡è¯•ï¼Œç­‰å¾… {sleep_time:.2f} ç§’... å¼‚å¸¸: {str(e)}")
                    time.sleep(sleep_time)
                    
                    # æŒ‡æ•°é€€é¿
                    delay *= backoff_factor
        return wrapper
    return decorator

class NewsURLs(BaseModel):
    """æ–°é—»URLåˆ—è¡¨è¾“å‡ºæ¨¡å‹"""
    urls: List[str] = Field(description="æ–°é—»æ–‡ç« çš„URLåˆ—è¡¨")

class NewsContent(BaseModel):
    """æ–°é—»å†…å®¹è¾“å‡ºæ¨¡å‹"""
    title: str = Field(description="æ–°é—»æ ‡é¢˜")
    summary: str = Field(description="æ–°é—»æ‘˜è¦ï¼Œä¸è¶…è¿‡4å¥è¯")
    tags: List[str] = Field(description="æ–°é—»æ ‡ç­¾åˆ—è¡¨")
    publish_date: str = Field(description="å‘å¸ƒæ—¥æœŸ")
    is_relevant: bool = Field(description="æ˜¯å¦ä¸ºç›¸å…³æ–°é—»")

class NewsCrawlerAgent:
    """æ™ºèƒ½æ–°é—»çˆ¬è™« Agent"""
    
    def __init__(self, db_url: str = None, tongyi_api_key: str = None):
        """åˆå§‹åŒ–çˆ¬è™« Agent
        
        Args:
            db_url: æ•°æ®åº“è¿æ¥URLï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            tongyi_api_key: AI APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
        """
        # ä»ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“é…ç½®
        if db_url is None:
            db_type = os.getenv("DB_TYPE", "mysql")
            db_host = os.getenv("DB_HOST", "127.0.0.1")
            db_port = os.getenv("DB_PORT", "3306")
            db_username = os.getenv("DB_USERNAME", "root")
            db_password = os.getenv("DB_PASSWORD")
            db_database = os.getenv("DB_DATABASE")
            
            if db_type == "mysql":
                db_url = f"mysql+pymysql://{db_username}:{db_password}@{db_host}:{db_port}/{db_database}"
            elif db_type == "postgresql":
                db_url = f"postgresql://{db_username}:{db_password}@{db_host}:{db_port}/{db_database}"
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")
                
        self.engine = create_engine(db_url)
        
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        if tongyi_api_key is None:
            tongyi_api_key = os.getenv("DASHSCOPE_API_KEY")
            
        self.llm = ChatTongyi(
            model="qwen-plus-latest",
            api_key=tongyi_api_key
        )
        
        # åˆå§‹åŒ–URLæå–çš„æç¤ºæ¨¡æ¿
        self.url_extract_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µåˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç»™å®šçš„HTMLå†…å®¹ä¸­æå–æ‰€æœ‰æ–°é—»æ–‡ç« çš„URLã€‚
            è¯·åˆ†æHTMLç»“æ„ï¼Œæ‰¾å‡ºæ‰€æœ‰æ–°é—»é“¾æ¥ã€‚é€šå¸¸è¿™äº›é“¾æ¥ä¼šåœ¨æ–°é—»åˆ—è¡¨ã€æ–‡ç« å¡ç‰‡æˆ–ç±»ä¼¼çš„å®¹å™¨ä¸­ã€‚
            åªè¿”å›æ–°é—»æ–‡ç« çš„URLï¼Œä¸è¦åŒ…å«å…¶ä»–ç±»å‹çš„é“¾æ¥ï¼ˆå¦‚å¯¼èˆªã€å¹¿å‘Šç­‰ï¼‰ã€‚
            è¯·åªè¿”å›ä»¥ä¸‹æ ¼å¼çš„ JSONï¼š
            {{
            "urls": ["url1", "url2", "url3", ..., "url10"]
            }}
            """),
            ("human", "{html_content}")
        ])
        
        # è‹±æ–‡å†…å®¹è¿‡æ»¤å’Œæå–çš„æç¤ºæ¨¡æ¿
        self.english_filter_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a professional news content filter and summarizer. Your task is to analyze news articles and filter for tax-related content.

Please filter out any unrelated news articles. Only include content related to tax Legislation, taxation Policy, HKICPA, or ACCA.

For each relevant article, provide:
1. A concise summary in English, no longer than 4 sentences, written in an objective and neutral tone
2. Tag each summary with one or more of the following categories: Legislation, Policy, HKICPA, ACCA
3. The publish date in YYYY-MM-DD format (extract from the HTML if clearly available, otherwise leave empty "")
4. Whether the article is relevant (true/false)

If the article is not relevant to tax matters, set is_relevant to false.

IMPORTANT: You must return ONLY valid JSON format. Do not include any additional text, explanation, or markdown formatting.

{{
    "title": "Article title",
    "summary": "Concise summary in English (max 4 sentences)",
    "tags": ["Legislation", "Policy", "HKICPA", "ACCA"],
    "publish_date": "",
    "is_relevant": true
}}
            """),
            ("human", "{html_content}")
        ])
        
        # ç¹ä½“ä¸­æ–‡å†…å®¹è¿‡æ»¤å’Œæå–çš„æç¤ºæ¨¡æ¿
        self.traditional_chinese_filter_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–°èå…§å®¹éæ¿¾å’Œæ‘˜è¦åŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯åˆ†ææ–°èæ–‡ç« ä¸¦ç¯©é¸å‡ºç¨…å‹™ç›¸é—œå…§å®¹ã€‚

è«‹éæ¿¾æ‰ä»»ä½•ç„¡é—œçš„æ–°èæ–‡ç« ã€‚åªåŒ…å«èˆ‡ç¨…å‹™ç«‹æ³•ã€ç¨…å‹™æ”¿ç­–ã€é¦™æ¸¯æœƒè¨ˆå¸«å…¬æœƒ(HKICPA)æˆ–ç‰¹è¨±å…¬èªæœƒè¨ˆå¸«å…¬æœƒ(ACCA)ç›¸é—œçš„å…§å®¹ã€‚

å°æ–¼æ¯ç¯‡ç›¸é—œæ–‡ç« ï¼Œè«‹æä¾›ï¼š
1. ç¹é«”ä¸­æ–‡çš„ç°¡æ½”æ‘˜è¦ï¼Œä¸è¶…é4å¥è©±ï¼Œä»¥å®¢è§€ä¸­æ€§çš„èªèª¿æ’°å¯«
2. ç‚ºæ¯å€‹æ‘˜è¦æ¨™è¨˜ä»¥ä¸‹ä¸€å€‹æˆ–å¤šå€‹é¡åˆ¥ï¼šç«‹æ³•ã€æ”¿ç­–ã€HKICPAã€ACCA
3. ç™¼ä½ˆæ—¥æœŸæ ¼å¼ç‚º YYYY-MM-DDï¼ˆå¦‚æœHTMLä¸­æœ‰æ˜ç¢ºä¿¡æ¯å‰‡æå–ï¼Œå¦å‰‡ç•™ç©º ""ï¼‰
4. æ–‡ç« æ˜¯å¦ç›¸é—œï¼ˆtrue/falseï¼‰

å¦‚æœæ–‡ç« èˆ‡ç¨…å‹™äº‹é …ç„¡é—œï¼Œè«‹è¨­ç½® is_relevant ç‚º falseã€‚

é‡è¦ï¼šå¿…é ˆåªè¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–æ–‡å­—ã€è§£é‡‹æˆ– markdown æ ¼å¼ã€‚

{{
    "title": "æ–‡ç« æ¨™é¡Œ",
    "summary": "ç¹é«”ä¸­æ–‡ç°¡æ½”æ‘˜è¦ï¼ˆæœ€å¤š4å¥è©±ï¼‰",
    "tags": ["ç«‹æ³•", "æ”¿ç­–", "HKICPA", "ACCA"],
    "publish_date": "",
    "is_relevant": true
}}
            """),
            ("human", "{html_content}")
        ])
        
        # ç®€ä½“ä¸­æ–‡å†…å®¹è¿‡æ»¤å’Œæå–çš„æç¤ºæ¨¡æ¿
        self.simplified_chinese_filter_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»å†…å®¹è¿‡æ»¤å’Œæ‘˜è¦åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ–°é—»æ–‡ç« å¹¶ç­›é€‰å‡ºç¨åŠ¡ç›¸å…³å†…å®¹ã€‚

è¯·è¿‡æ»¤æ‰ä»»ä½•æ— å…³çš„æ–°é—»æ–‡ç« ã€‚åªåŒ…å«ä¸ç¨åŠ¡ç«‹æ³•ã€ç¨åŠ¡æ”¿ç­–ã€é¦™æ¸¯ä¼šè®¡å¸ˆå…¬ä¼š(HKICPA)æˆ–ç‰¹è®¸å…¬è®¤ä¼šè®¡å¸ˆå…¬ä¼š(ACCA)ç›¸å…³çš„å†…å®¹ã€‚

å¯¹äºæ¯ç¯‡ç›¸å…³æ–‡ç« ï¼Œè¯·æä¾›ï¼š
1. ç®€ä½“ä¸­æ–‡çš„ç®€æ´æ‘˜è¦ï¼Œä¸è¶…è¿‡4å¥è¯ï¼Œä»¥å®¢è§‚ä¸­æ€§çš„è¯­è°ƒæ’°å†™
2. ä¸ºæ¯ä¸ªæ‘˜è¦æ ‡è®°ä»¥ä¸‹ä¸€ä¸ªæˆ–å¤šä¸ªç±»åˆ«ï¼šç«‹æ³•ã€æ”¿ç­–ã€HKICPAã€ACCA
3. å‘å¸ƒæ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DDï¼ˆå¦‚æœHTMLä¸­æœ‰æ˜ç¡®ä¿¡æ¯åˆ™æå–ï¼Œå¦åˆ™ç•™ç©º ""ï¼‰
4. æ–‡ç« æ˜¯å¦ç›¸å…³ï¼ˆtrue/falseï¼‰

å¦‚æœæ–‡ç« ä¸ç¨åŠ¡äº‹é¡¹æ— å…³ï¼Œè¯·è®¾ç½® is_relevant ä¸º falseã€‚

é‡è¦ï¼šå¿…é¡»åªè¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—ã€è§£é‡Šæˆ– markdown æ ¼å¼ã€‚

{{
    "title": "æ–‡ç« æ ‡é¢˜",
    "summary": "ç®€ä½“ä¸­æ–‡ç®€æ´æ‘˜è¦ï¼ˆæœ€å¤š4å¥è¯ï¼‰",
    "tags": ["ç«‹æ³•", "æ”¿ç­–", "HKICPA", "ACCA"],
    "publish_date": "",
    "is_relevant": true
}}
            """),
            ("human", "{html_content}")
        ])
        
        # è®¾ç½®è¾“å‡ºè§£æå™¨
        self.url_parser = JsonOutputParser(pydantic_schema=NewsURLs)
        self.content_parser = JsonOutputParser(pydantic_schema=NewsContent)
        
        # æ„å»ºå¤„ç†é“¾
        self.url_chain = (
            {"html_content": RunnablePassthrough()} 
            | self.url_extract_prompt 
            | self.llm 
            | self.url_parser
        )
        
        # æ„å»ºä¸åŒè¯­è¨€çš„å†…å®¹å¤„ç†é“¾
        self.english_chain = (
            {"html_content": RunnablePassthrough()} 
            | self.english_filter_prompt 
            | self.llm 
            | self.content_parser
        )
        
        self.traditional_chinese_chain = (
            {"html_content": RunnablePassthrough()} 
            | self.traditional_chinese_filter_prompt 
            | self.llm 
            | self.content_parser
        )
        
        self.simplified_chinese_chain = (
            {"html_content": RunnablePassthrough()} 
            | self.simplified_chinese_filter_prompt 
            | self.llm 
            | self.content_parser
        )
    
    def get_content_chain_by_language(self, language: str):
        """æ ¹æ®è¯­è¨€è·å–å¯¹åº”çš„å†…å®¹å¤„ç†é“¾"""
        language_chains = {
            'en': self.english_chain,                    # è‹±æ–‡
            'zh-hk': self.traditional_chinese_chain,     # ç¹ä½“ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰
            'zh': self.simplified_chinese_chain          # ç®€ä½“ä¸­æ–‡
        }
        return language_chains.get(language, self.simplified_chinese_chain)
    
    @retry_decorator(max_retries=3, exceptions=(requests.RequestException, requests.Timeout, ConnectionError))
    def fetch_html(self, url: str) -> str:
        """åŒæ­¥è·å–ç½‘é¡µHTMLå†…å®¹ï¼Œå¤±è´¥æ—¶ä¼šè‡ªåŠ¨é‡è¯•"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        
        response = requests.get(url, timeout=30, headers=headers, verify=False)
        if response.status_code == 200:
            response.encoding = response.apparent_encoding or 'utf-8'
            return response.text
        else:
            logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: HTTP {response.status_code}")
            logger.debug(f"Response headers: {response.headers}")
            logger.debug(f"Response content: {response.text[:500]}...")
            raise requests.RequestException(f"HTTPé”™è¯¯ {response.status_code}")
            
    def normalize_urls(self, urls: List[str], base_url: str) -> List[str]:
        """æ ‡å‡†åŒ–URLåˆ—è¡¨ï¼Œå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„"""
        normalized_urls = []
        
        for url in urls:
            if not url:
                continue
                
            # å¦‚æœå·²ç»æ˜¯å®Œæ•´URLï¼Œç›´æ¥ä½¿ç”¨
            if url.startswith(('http://', 'https://')):
                normalized_urls.append(url)
            else:
                # ä½¿ç”¨urljoinå¤„ç†ç›¸å¯¹è·¯å¾„
                full_url = urljoin(base_url, url)
                normalized_urls.append(full_url)
                
        return normalized_urls
    
    def extract_base_url(self, url: str) -> str:
        """ä»å®Œæ•´URLä¸­æå–åŸºç¡€URL"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    
    def extract_news_urls(self, html_content: str, source_url: str) -> List[str]:
        """ä½¿ç”¨LLMæå–æ–°é—»URLåˆ—è¡¨ï¼Œå¹¶å¤„ç†ç›¸å¯¹è·¯å¾„"""
        try:
            result = self.url_chain.invoke(html_content)
            raw_urls = result['urls']
            
            # æå–åŸºç¡€URL
            base_url = self.extract_base_url(source_url)
            
            # æ ‡å‡†åŒ–URL
            normalized_urls = self.normalize_urls(raw_urls, base_url)
            
            logger.info(f"æå–åˆ° {len(raw_urls)} ä¸ªURLï¼Œæ ‡å‡†åŒ–å {len(normalized_urls)} ä¸ª")
            if len(raw_urls) > 0 and len(normalized_urls) > 0:
                logger.debug(f"ç¤ºä¾‹URLè½¬æ¢: {raw_urls[0]} -> {normalized_urls[0]}")
            
            return normalized_urls
            
        except Exception as e:
            logger.error(f"æå–URLå¤±è´¥: {str(e)}")
            return []
            
    def clean_json_response(self, response_text: str) -> str:
        """æ¸…ç†LLMå“åº”ï¼Œæå–æœ‰æ•ˆçš„JSONéƒ¨åˆ†"""
        # ç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
        response_text = re.sub(r'```json\s*', '', response_text)
        response_text = re.sub(r'```\s*$', '', response_text)
        
        # å¯»æ‰¾JSONå¯¹è±¡
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            return json_match.group(0)
        
        return response_text.strip()
    
    def get_valid_tags_by_language(self, language: str) -> List[str]:
        """æ ¹æ®è¯­è¨€è·å–æœ‰æ•ˆçš„æ ‡ç­¾åˆ—è¡¨"""
        tag_mappings = {
            'en': ['Legislation', 'Policy', 'HKICPA', 'ACCA'],
            'zh-hk': ['ç«‹æ³•', 'æ”¿ç­–', 'HKICPA', 'ACCA'],  # ç¹ä½“ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰
            'zh': ['ç«‹æ³•', 'æ”¿ç­–', 'HKICPA', 'ACCA']      # ç®€ä½“ä¸­æ–‡
        }
        return tag_mappings.get(language, ['ç«‹æ³•', 'æ”¿ç­–', 'HKICPA', 'ACCA'])
    
    def validate_tags(self, tags: List[str], language: str) -> List[str]:
        """éªŒè¯å’Œè¿‡æ»¤æ ‡ç­¾ï¼Œåªä¿ç•™æœ‰æ•ˆçš„æ ‡ç­¾"""
        valid_tags = self.get_valid_tags_by_language(language)
        valid_tags_lower = [tag.lower() for tag in valid_tags]
        
        filtered_tags = []
        for tag in tags:
            if not tag:
                continue
                
            tag_clean = tag.strip()
            
            # ç²¾ç¡®åŒ¹é…
            if tag_clean in valid_tags:
                filtered_tags.append(tag_clean)
                continue
            
            # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
            tag_lower = tag_clean.lower()
            for i, valid_tag_lower in enumerate(valid_tags_lower):
                if tag_lower == valid_tag_lower:
                    filtered_tags.append(valid_tags[i])  # ä½¿ç”¨åŸå§‹å¤§å°å†™
                    break
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_tags = []
        for tag in filtered_tags:
            if tag not in seen:
                seen.add(tag)
                unique_tags.append(tag)
        
        if len(unique_tags) != len(tags):
            logger.debug(f"æ ‡ç­¾è¿‡æ»¤: åŸå§‹æ ‡ç­¾ {tags} -> æœ‰æ•ˆæ ‡ç­¾ {unique_tags}")
        
        return unique_tags
    
    def validate_and_fix_result(self, result: Dict, language: str = 'zh') -> Dict:
        """éªŒè¯å’Œä¿®å¤æå–ç»“æœ"""
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        validated_result = {
            "title": result.get("title", ""),
            "summary": result.get("summary", ""),
            "tags": result.get("tags", []),
            "publish_date": result.get("publish_date", ""),
            "is_relevant": result.get("is_relevant", False)
        }
        
        # ç¡®ä¿tagsæ˜¯åˆ—è¡¨
        if not isinstance(validated_result["tags"], list):
            if isinstance(validated_result["tags"], str):
                validated_result["tags"] = [validated_result["tags"]]
            else:
                validated_result["tags"] = []
        
        # éªŒè¯å’Œè¿‡æ»¤æ ‡ç­¾
        validated_result["tags"] = self.validate_tags(validated_result["tags"], language)
        
        # ç¡®ä¿is_relevantæ˜¯å¸ƒå°”å€¼
        if not isinstance(validated_result["is_relevant"], bool):
            validated_result["is_relevant"] = str(validated_result["is_relevant"]).lower() in ['true', '1', 'yes']
        
        return validated_result
    
    def extract_news_content(self, html_content: str, language: str = 'zh') -> Dict:
        """ä½¿ç”¨LLMæå–å’Œè¿‡æ»¤æ–°é—»å†…å®¹ï¼Œå¸¦æœ‰é‡è¯•å’Œå®¹é”™æœºåˆ¶"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # æ ¹æ®è¯­è¨€é€‰æ‹©å¯¹åº”çš„å¤„ç†é“¾
                content_chain = self.get_content_chain_by_language(language)
                
                # å°è¯•ä½¿ç”¨ LangChain å¤„ç†é“¾
                try:
                    result = content_chain.invoke(html_content)
                    result = self.validate_and_fix_result(result, language)
                except Exception as chain_error:
                    logger.warning(f"LangChainå¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(chain_error)}")
                    
                    # å¦‚æœLangChainå¤±è´¥ï¼Œå°è¯•ç›´æ¥è°ƒç”¨LLM
                    try:
                        # è·å–å¯¹åº”è¯­è¨€çš„promptæ¨¡æ¿
                        if language == 'en':
                            prompt_template = self.english_filter_prompt
                        elif language == 'zh-hk':
                            prompt_template = self.traditional_chinese_filter_prompt
                        else:  # zh é»˜è®¤ç®€ä½“ä¸­æ–‡
                            prompt_template = self.simplified_chinese_filter_prompt
                        
                        formatted_prompt = prompt_template.format(html_content=html_content)
                        
                        # ç›´æ¥è°ƒç”¨LLM
                        raw_response = self.llm.invoke(formatted_prompt)
                        response_text = raw_response.content if hasattr(raw_response, 'content') else str(raw_response)
                        
                        # æ¸…ç†å“åº”æ–‡æœ¬
                        cleaned_response = self.clean_json_response(response_text)
                        
                        # å°è¯•è§£æJSON
                        result = json.loads(cleaned_response)
                        result = self.validate_and_fix_result(result, language)
                        
                    except Exception as direct_error:
                        logger.warning(f"ç›´æ¥LLMè°ƒç”¨ä¹Ÿå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(direct_error)}")
                        if attempt < max_retries - 1:
                            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                            continue
                        else:
                            logger.error("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                            return None
                
                # æ£€æŸ¥å¿…è¦å­—æ®µ
                if not result.get('title') or not result.get('summary'):
                    logger.warning(f"æå–å†…å®¹ç¼ºå°‘å¿…è¦å­—æ®µ (å°è¯• {attempt + 1}/{max_retries}): {result.keys()}")
                    if attempt < max_retries - 1:
                        time.sleep(1)
                        continue
                    else:
                        return None
                
                # å¦‚æœä¸ç›¸å…³ï¼Œè¿”å›None
                if not result.get('is_relevant', False):
                    logger.debug(f"æ–°é—»ä¸ç›¸å…³ï¼Œå·²è¿‡æ»¤: {result.get('title', 'Unknown')}")
                    return None
                
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œä¹Ÿè§†ä¸ºä¸ç›¸å…³
                if not result.get('tags') or len(result.get('tags', [])) == 0:
                    logger.debug(f"æ–°é—»æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œå·²è¿‡æ»¤: {result.get('title', 'Unknown')}")
                    return None
                    
                # å¤„ç†å¯èƒ½ç¼ºå¤±çš„å‘å¸ƒæ—¥æœŸ
                if not result.get('publish_date'):
                    logger.debug("æœªæå–åˆ°å‘å¸ƒæ—¥æœŸï¼Œå°†åœ¨ä¿å­˜æ—¶ä½¿ç”¨å½“å‰æ—¥æœŸ")
                    
                return {
                    "title": result['title'],
                    "summary": result['summary'],
                    "tags": result.get('tags', []),
                    "publish_date": result.get('publish_date', '')
                }
                
            except Exception as e:
                logger.error(f"æå–å†…å®¹å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    logger.error("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    return None
        
        return None
            
    def check_urls_exist(self, urls: List[str]) -> List[str]:
        """æ‰¹é‡æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œè¿”å›ä¸å­˜åœ¨çš„URLåˆ—è¡¨"""
        if not urls:
            return []
            
        existing_url_set = set()
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…SQLæŸ¥è¯¢è¿‡é•¿ï¼ˆæ¯æ‰¹æœ€å¤š50ä¸ªURLï¼‰
        batch_size = 50
        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i + batch_size]
            
            # æ„å»ºæŸ¥è¯¢è¯­å¥
            placeholders = ", ".join([f":url_{j}" for j in range(len(batch_urls))])
            check_query = text(f"""
                SELECT url FROM news 
                WHERE url IN ({placeholders})
            """)
            
            # æ„å»ºå‚æ•°å­—å…¸
            params = {f"url_{j}": url for j, url in enumerate(batch_urls)}
            
            try:
                with self.engine.connect() as conn:
                    existing_urls = conn.execute(check_query, params).fetchall()
                    existing_url_set.update(row[0] for row in existing_urls)
            except Exception as e:
                logger.error(f"æ£€æŸ¥URLæ‰¹æ¬¡å¤±è´¥: {str(e)}")
                continue
            
        # è¿”å›ä¸å­˜åœ¨çš„URL
        new_urls = [url for url in urls if url not in existing_url_set]
        
        logger.info(f"URLæ£€æŸ¥ç»“æœ: æ€»å…±{len(urls)}ä¸ª, å·²å­˜åœ¨{len(existing_url_set)}ä¸ª, æ–°å¢{len(new_urls)}ä¸ª")
        return new_urls

    def save_to_db(self, news_data: Dict, source: Dict):
        """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“"""
        if not news_data:
            return
            
        # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if not news_data.get("publish_date"):
            current_date = datetime.now().strftime("%Y-%m-%d")
            news_data["publish_date"] = current_date
            logger.debug(f"æœªæ‰¾åˆ°å‘å¸ƒæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ: {current_date}")
        
        # å°†æ ‡ç­¾è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        tags_str = ", ".join(news_data.get("tags", []))
            
        data = {
            "language": source["language"],
            "source": source["source"],
            "date": news_data["publish_date"],
            "content": news_data["summary"],  # ä½¿ç”¨æ‘˜è¦è€Œä¸æ˜¯å®Œæ•´å†…å®¹
            "url": source["url"],
            "title": news_data["title"],
            "tags": tags_str
        }
        
        # ç›´æ¥æ’å…¥æ•°æ®ï¼ˆå› ä¸ºURLå·²ç»åœ¨çˆ¬å–å‰æ£€æŸ¥è¿‡äº†ï¼‰
        insert_query = text("""
            INSERT INTO news (language, source, date, content, url, title, news_type)
            VALUES (:language, :source, :date, :content, :url, :title, :tags)
        """)
        
        try:
            with self.engine.connect() as conn:
                conn.execute(insert_query, data)
                conn.commit()
                logger.info(f"æˆåŠŸä¿å­˜æ–°é—»: {data['title']} - æ ‡ç­¾: {tags_str}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°é—»å¤±è´¥ {data['url']}: {str(e)}")
                
    def process_news_url(self, url: str, source: Dict):
        """å¤„ç†å•ä¸ªæ–°é—»URL"""
        try:
            html_content = self.fetch_html(url)
            if html_content:
                news_data = self.extract_news_content(html_content, source["language"])
                if news_data:   # åªæœ‰ç›¸å…³æ–°é—»æ‰ä¼šä¿å­˜
                    source_with_url = dict(source)
                    source_with_url["url"] = url
                    self.save_to_db(news_data, source_with_url)
        except Exception as e:
            logger.error(f"å¤„ç†æ–°é—»URLå¤±è´¥ {url}: {str(e)}")
                
    def crawl_news(self, source: Dict):
        """çˆ¬å–å•ä¸ªæ¥æºçš„æ–°é—»
        
        Args:
            source: åŒ…å«urlã€languageç­‰ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # è·å–ä¸»é¡µHTML
            html_content = self.fetch_html(source["url"])
            if not html_content:    # TODO å‡ºé”™å¤„ç†
                return
                
            # æå–æ–°é—»URLåˆ—è¡¨ï¼Œå¹¶å¤„ç†ç›¸å¯¹è·¯å¾„
            news_urls = self.extract_news_urls(html_content, source["url"])
            if not news_urls:
                logger.warning(f"æœªæå–åˆ°æ–°é—»URL: {source['source']}")
                return
                
            # æ£€æŸ¥å“ªäº›URLæ˜¯æ–°çš„ï¼ˆæœªçˆ¬å–è¿‡çš„ï¼‰
            new_urls = self.check_urls_exist(news_urls)
            if not new_urls:
                logger.info(f"æ‰€æœ‰æ–°é—»URLéƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡: {source['source']}")
                return
                
            logger.info(f"å¼€å§‹å¤„ç† {len(new_urls)} ä¸ªæ–°URLï¼Œæ¥æº: {source['source']}")
            
            # ä¸²è¡Œå¤„ç†æ–°çš„æ–°é—»URL
            for url in new_urls:
                try:
                    self.process_news_url(url, source)
                    # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    time.sleep(1)
                except Exception as e:
                    logger.error(f"å¤„ç†æ–°é—»URLå¤±è´¥ {url}: {str(e)}")
                    continue
                
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {source['source']}: {str(e)}")
            
    def get_source_list(self) -> List[Dict]:
        """ä»æ•°æ®åº“è·å–éœ€è¦çˆ¬å–çš„ç½‘ç«™åˆ—è¡¨"""
        query = text("""
            SELECT url, language, source_name, info 
            FROM news_sources 
            WHERE is_active = true
        """)
        
        with self.engine.connect() as conn:
            results = conn.execute(query).fetchall()
            
        return [
            {
                "url": row[0],
                "language": row[1],
                "source": row[2],
                "info": row[3]
            }
            for row in results
        ]
        
    def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰æ¥æºçš„æ–°é—»"""
        sources = self.get_source_list()
        logger.info(f"æ‰¾åˆ° {len(sources)} ä¸ªæ–°é—»æºéœ€è¦å¤„ç†")
        
        for i, source in enumerate(sources, 1):
            try:
                logger.info(f"[{i}/{len(sources)}] å¼€å§‹å¤„ç†æ–°é—»æº: {source['source']}")
                self.crawl_news(source)
                logger.info(f"[{i}/{len(sources)}] å®Œæˆå¤„ç†æ–°é—»æº: {source['source']}")
            except Exception as e:
                logger.error(f"çˆ¬å–æ¥æºå¤±è´¥ {source['source']}: {str(e)}")
                continue


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    import os
    
    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    api_key = os.getenv("DASHSCOPE_API_KEY")
    
    if not api_key:
        logger.error("âŒ é”™è¯¯: è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        logger.error("ç¤ºä¾‹: export DASHSCOPE_API_KEY='your_api_key_here'")
        return
    
    # æ£€æŸ¥æ•°æ®åº“é…ç½®
    db_type = os.getenv("DB_TYPE", "mysql")
    db_host = os.getenv("DB_HOST")
    db_database = os.getenv("DB_DATABASE")
    
    if not db_host or not db_database:
        logger.error("âŒ é”™è¯¯: è¯·ç¡®ä¿è®¾ç½®äº†æ•°æ®åº“ç¯å¢ƒå˜é‡")
        logger.error("éœ€è¦çš„å˜é‡: DB_TYPE, DB_HOST, DB_DATABASE, DB_USERNAME, DB_PASSWORD")
        return
    
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ–°é—»çˆ¬è™«...")
    logger.info(f"ğŸ“Š ä½¿ç”¨æ•°æ®åº“: {db_type}://{db_host}/{db_database}")
    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: qwen-plus-latest")
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        agent = NewsCrawlerAgent()
        
        logger.info("âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸï¼")
        logger.info("ğŸ“° å¼€å§‹æ‰§è¡Œæ–°é—»çˆ¬å–ä»»åŠ¡...")
        logger.info("=" * 50)
        
        start_time = datetime.now()
        
        # æ‰§è¡Œçˆ¬å–
        agent.crawl_all()
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("=" * 50)
        logger.info(f"ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration}")
        logger.info(f"ğŸ“… å®Œæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸  ç”¨æˆ·ä¸­æ–­äº†çˆ¬å–ä»»åŠ¡")
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 